services:
  vllm:
    container_name: vllm
    build:
      context: .
      dockerfile: Dockerfile.vllm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["2"]
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN=
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - NCCL_IB_DISABLE=1
      - LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/lib/x86_64-linux-gnu
    command:
      - "--model=Qwen/Qwen2.5-7B-Instruct"
      - "--dtype=bfloat16"
      - "--tensor-parallel-size=1"
      - "--gpu-memory-utilization=0.7"
      - "--max-model-len=4096"
      - "--port=8000"
      - "--enforce-eager"
    ports: ["8000:8000"]
    ipc: host
    volumes:
      - hf_cache:/root/.cache/huggingface
  rag-examplecompany:
    container_name: rag-examplecompany
    build: ./rag
    command: ["uvicorn","app:app","--host","0.0.0.0","--port","8080"]
    environment:
      - TENANT_ID=examplecompany
      - VLLM_BASE_URL=http://vllm:8000/v1
    volumes:
      - ./data/example_company:/app/data:ro
      - indices_examplecompany:/app/indices
    ports: ["8101:8080"]

  rag-testcompany:
    container_name: rag-testcompany
    build: ./rag
    command: ["uvicorn","app:app","--host","0.0.0.0","--port","8080"]
    environment:
      - TENANT_ID=testcompany
      - VLLM_BASE_URL=http://vllm:8000/v1
    volumes:
      - ./data/test_company:/app/data:ro
      - indices_testcompany:/app/indices
    ports: ["8102:8080"]

  stt:
    container_name: stt
    build: ./stt
    gpus:
      - driver: nvidia
        device_ids: ["3"]
        capabilities: [gpu]
    environment:
      - HF_HOME=/root/.cache/huggingface
      - HUGGING_FACE_HUB_TOKEN=
      - PYTORCH_ALLOC_CONF=expandable_segments:True
      - STT_MODEL_ID=large-v3
      - STT_LANGUAGE=hu
    volumes:
      - hf_cache:/root/.cache/huggingface
    ports: ["5001:5001"]

  tts:
    container_name: tts
    build: ./tts
    environment:
      - PIPER_VOICE=/voices/hu_HU-berta-medium.onnx
      - PIPER_VOICE_JSON=/voices/hu_HU-berta-medium.onnx.json
      - TTS_DEVICE=cuda
    ports: ["5002:5002"]

  vad:
    container_name: vad
    build: ./vad
    ports: ["19001:9001"]

  web-testcompany:
    container_name: web-testcompany
    build: ./web
    environment:
      - STT_URL=http://stt:5001/transcribe
      - RAG_URL=http://rag-testcompany:8080/query
      - TTS_URL=http://tts:5002/speak
      - VAD_URL=http://vad:9001
      - COMPANY=testcompany
      - PORT=8090
    depends_on:
      - vad
    ports: ["8090:8090"]

  web-frontend-testcompany:
    container_name: web-frontend-testcompany
    build: ./web/frontend
    environment:
      - API_BASE=http://localhost:8090
      - TITLE=Voice Assistant UI (testcompany)
      - PORT=8080
    depends_on:
      - web-testcompany
    ports: ["8080:8080"]

  web-examplecompany:
    container_name: web-examplecompany
    build: ./web
    environment:
      - STT_URL=http://stt:5001/transcribe
      - RAG_URL=http://rag-examplecompany:8080/query
      - TTS_URL=http://tts:5002/speak
      - VAD_URL=http://vad:9001
      - COMPANY=examplecompany
      - PORT=8091
    depends_on:
      - vad
    ports: ["8091:8091"]

  web-frontend-examplecompany:
    container_name: web-frontend-examplecompany
    build: ./web/frontend
    environment:
      - API_BASE=http://localhost:8091
      - TITLE=Voice Assistant UI (examplecompany)
      - PORT=8081
    depends_on:
      - web-examplecompany
    ports: ["8081:8081"]

volumes:
  hf_cache:
  indices_examplecompany:
  indices_testcompany:
